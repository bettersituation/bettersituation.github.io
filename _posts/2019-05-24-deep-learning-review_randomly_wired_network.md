---
layout: post
topic: deep-learning
title: 주관적인 논문리뷰 - Exploring Randomly Wired Neural Networks for Image Recognition - FAIR
---
[실제 논문은 여기로](https://arxiv.org/abs/1904.01569)

#### 1. 대략적인 논문 설명  

우리가 딥러닝이다 뉴럴 네트워크다 말은 하고 있지만, 사실 아직 파헤치지 못한 것들이 얼마나 많겠는가?  
자기 인생을 걸고 batch size 는 얼마가 좋다, optimizer 는 어떤게 좋다, learning rate 는 어떻게 해야지 좋다 라고 말할 수 있는 사람이 있는가?  
없다.  

이거는 구글, 아마존, openAI, 딥마인드 등 유명한 회사와 회사에 다니는 사람도 확신을 갖고 말할 수 없는 것이다.  
그래서 어차피 답도 없는거 대충 알아서 학습해서 대충 알아서 좋은 설정을 찾아주면 좋겠다 라는 심정에서 출발한 것이 이 쪽 분야이다.  
이 논문 역시 그 흐름을 이어가고 있다. 다만 학습해서 최적의 구조를 찾는것까지 나아가진 않았고, 랜덤한 네트워크 구조의 결과를 확인하고 가능성을 점치는 것에서 논문의 내용은 마무리된다.  

결과는 어떨까? 좋다. 좋으니까 논문을 썼을 것이다. 막 말도 안되게 좋다 그런건 아니지만 인간이 열심히 만들었던 구조들(ex. ResNet) 만큼, 또는 약간 상회하여 좋다.  
그러면 랜덤 네트워크의 구조는 어떻게 만들었을까? 컴퓨터과학 분야에서 말하는 그래프를 랜덤으로 생성하는 기존 모델을 그대로 차용해왔다.  

논문 설명 끝~

---

#### 2. 주관적인 논문 해석

논문 초반에 저자들은 완벽한 랜덤 네트워크를 구성한 것은 아니라고 밝힌다.  
저자들조차 일정 부분 주관적인 부분이 개입되어 있음을 인정했는데, 나는 이 부분의 영향력이 꽤나 클 것으로 생각한다. 네트워크 구조에 대한 부분만 보면  

1) ResNet 등에서 쓰던 필터 사이즈와 채널 개수 조정 기교를 그대로 사용한다.  
ResNet 등에서는 적당히 레이어를 지날 때마다 필터사이즈를 1/2로 줄이고, 채널 개수를 2배 늘리는 기교를 사용하는데, 해당 논문에서도 마찬가지로 이 기교를 무조건 사용한다.

2) Batch normalization 을 무조건 적용했다.

3) 레이어의 단방향 흐름을 가정한다.

나는 위에서 말한 가정들의 영향력이 꽤 컸을 것으로 추측한다. 그리고 또한 논문에서 제시한 그림(Figure3, Figure4) 를 보게 되면,  
구조도 구조겠지만 결국 어떻게 skip connection 이 구성되어서 gradient, 또는 feature extraction 이 잘 흐를 수 있게 만들 수 있을까 하는 관점이 더 중요하게 느껴진다.


---

#### 3. 결론

1) skip connection 을 잘 활용하여 흐름이 원활하게 흐를 수 있도록 만들자  
2) 아직은 갈 길이 멀다.  
3) 나도 열심히 해야겠다.
