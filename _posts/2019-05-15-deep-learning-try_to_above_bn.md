---
layout: post
topic: deep-learning
title: 배치 노말라이제이션을 응용해보고 싶었는데 실패했지만 얻은 것은 있다.
---
#### 해당 프로젝트를 보고 싶다면 [이곳으로](https://github.com/bettersituation/rigid_batch_norm)
---
결론부터 말하자면 망했다. 일반적인 배치 노말라이제이션에 비해 이렇다할 좋은 결과나 특성을 찾지 못했다. 그래도 한 번 얘기는 해보고자 한다.


배치 노말라이제이션은 배치에서 나온 각 레이어의 값들(activation 전) 을 해당 레이어의 평균과 분산으로 표준화한 후 activation 을 실행하는 것을 말한다. 이렇게 함으로써 안정적인 학습이 가능하다. 논문에서 얘기하는 covariate shift 가 제거되기 때문이다. [논문은 여기](https://arxiv.org/abs/1502.03167)

물론 그 뿐만 아니라 다른 요인도 작용했을 것이다. 여러 이유가 있겠으나 내 생각에는 요즘 표준적으로 사용되는 activation 이 Rectifer Linear Unit (ReLU) 에 기반을 둔다는 것, 표준화 및 $\gamma$ 항의 존재로 편미분 값이 보다 안정적으로 형성된다는 것 또한 긍정적인 결과를 만들어내는 요인으로써 작용했을 것이다.

어쨌든 저쨌든 간에 배치 노말라이제이션이 결국은 정규화 해주는 작업이므로, 정규화 방법을 약간 수정하여 더 좋은 결과를 얻을 수 있지 않을까 생각했더랬다.

결론은 위에서 말했다시피 좋지 않았다. 별 장점이 보이지 않았다. 그래도 무엇을 해보고자 했는지 작성해둔다.

---
첫 번째로는 정규화 작업에서 특이값, 즉 너무 크거나 너무 작은 값의 영향을 없애보고자 했다. 이러한 특이값은 정규화된 값을 보고 판단할 수 있다. 그렇다면 특정 boundary 에서 벗어나는 값은 제외하고 다시 boundary 안에 있는 값들만을 사용하여 평균 및 분산을 구할 수 있다.
이렇게 구한 평균과 분산을 사용하여 정규화 해주는 방법을 떠올렸다. (해당 boundary 안에 속하는 데이터의 비율은 체비셰프 부등식으로부터 보장될 수 있다. [체비셰프 부등식 보기](https://bettersituation.github.io/2019/05/01/miscs-markov-chevyshev-inequality.html))

그러나 이는 좋은 결과를 낳지 못했다. 그 이유는 내가 추측하기로는 다음과 같다.
1) 특이값의 존재 및 영향력
2) 특정 boundary 를 벗어나는 값을 regularization 으로 사용한다면, 해당 term 의 불안정성
3) regularization 이 안정적이지 못하므로 높은 learning rate 적용 시 그래디언트 폭발
4) regularization 에 의한 레이어 표현력 제어

---

위 아이디어로 좋은 결과를 얻지 못해서, 다른 방법을 떠올려보았다. 바로 쿨백-라이블러 발산을 활용하는 것이다. 만약 각 레이어에 해당되는 인풋 값이 정규분포를 따를 수 있다고 가정한다면, 표준 정규분포와의 쿨백-라이블러 발산을 regularization 으로 활용할 수 있지 않을까 하는 생각이었다. 즉 배치의 평균과 분산을 바탕으로, 정규분포를 따른다는 가정 하에, 표준 정규분포와의 쿨백-라이블러 발산을 regularization 으로 주는 것이다. 이 아이디어 역시 딱히 좋은 결과를 얻지 못했다. 추측으로는
1) 당연하게도 각 레이어의 인풋 값은 정규분포를 따르지 않는다. 특히 ReLU 와 같은 activation을 사용할 때는 더욱 두드러질 것이다.
2) 쿨백-라이블러 발산이 특정 수준 이상으로 감소하지 않는다.
3) 독립적인 정규분포의 가중합 역시 정규분포이다. 따라서 만약 쿨백-라이블러 발산을 안정적으로 쓰고 싶다면, 적합한 범위 안에서 initialize 해줘야 한다. 물론 xavier 등의 initializer 역시 노드 개수에 의해 결정되나, 과연 적합하다고 할 수 있을지 의문이다.
4) 배치 노말라이제이션에서의 $\gamma$ (인풋의 스케일을 결정하는 값) 처럼, 그래디언트 역전파 시 그래디언트를 안정화시켜주는 term 이 존재하지 않는다.

---

실험은 실패했지만 외적으로 얻은 것은 있더랬다. 배치 노말라이제이션에 대해 다시 한 번 생각해볼 수 있는 기회이기도 했고..

그 외에 얻은 교훈은 물론 있다.
첫 쨰로는 비교 가능성이다. 일반적으로 실험에서 많이 쓰는 ResNet 이나 VGG, GoogLeNet 처럼 표준적인 모델을 사용해야 다른 논문과의 비교가 쉬울 것이다. 그런 의미에서 표준적인 모델을 무엇을 쓸지 고민을 꽤 했었다.

데이터 셋도 마찬가지다. 일반적으로 성능 확인을 위해 사용하는 데이터셋을 활용해서 비교해야겠구나 싶더라.

두 번째로 느낀 것은 확인 가능한 값들에 대한 추적이다. Loss 랑 Accuracy 만 확인하는 것은 나에게 맞지 않는거 같다. 맘같아서야 모든 값들을 하나하나 확인하고 싶지만 정말 작은 형태의 Resnet 도 변수의 개수가 10 만개가 넘는다. 그래서 솔직히 하나하나는 다 못볼 것 같다. 그래도 적어도 레이어 별로 해당 변수의 값이나 그래디언트 값을 확인할 필요는 있을거같다. 그래야 조금이라도 덜 답답하지 않을까

세 번째로 모델 구축에서 내가 선택할 수 있는 것이 너무 많다. optimzer 를 SGD 에서 ADAM 으로 바꾸니까 에측률이 10% 가 상승했었다. 이게 뭐라고 이렇게 큰 차이가 나도 되나 싶더라. 레이어별 초기값 및 activation, learning rate 등 내가 시도하지 않은 최상의 조합은 얼마나 많을 것인가 여간 두려운 것이 아니었다. 이래서 AutoML 이 나오는구나 싶더라.

네 번째는 Data Augmentation 이다. cifar10 의 경우 학습 데이터가 5만개 있는데, 모델이 5만개 범주를 다 맞췄다. 물론 test 의 예측률은 좋지 않았다. 5만개 데이터에 대해서도 이렇게 쉽게 overfitting 이 되는데, 데이터가 더 적으면 얼마나 더 오버피팅이 잘 될까 하는 생각이 들었다. (파라미터 개수가 10만개 넘으니 어떻게 보면 오버피팅 되는게 당연한 것일 수 있지만)
이를 방지하기 위해서 사진을 돌리고 자르고 어떻게든 조금씩 다르게 해서 인풋으로 넣어주는구나 싶었다.

다섯 째는 빠른 학습의 필요성이다. 컴퓨터 연산능력이 빠방한 것이 아니니, 학습이 되는 과정이 느리니 답답하기 그지 없다. multi-gpu, 병렬 연산 등 이것들이 다 어떻게든 빨리 모델을 학습시켜서 결과를 얻어보겠단 것인데, 크게 공감 못하다가 이제야 공감했다. 그리고 메모리와 GPU 메모리도 문제이다. 그냥 메모리야 좀 커서 괜찮았는데, GPU에 테스트 데이터를 다 넣으려고 하면 메모리가 모자라더라.

여셧 째는 그래서 코딩을 잘할 수 있어야겠다는 생각이 들었다. 그래야 빠르게 작성하고 빠르게 돌릴 수 있으니까. c++ 이나 cuda 에 직접 접근하여 사용하면 빠를라나 싶지만 아직 능력이 안되므로 일단 패스. 추후를 기약하겠다.

일곱 째는 scalable 이란 용어에 대한 생각이다. 대충 해석하면 확장 가능한 같은 뜻이 되겠는데, 큰 데이터와 큰 규모의 모델에도 사용할 수 있는 방법론이면 좋겠다는 생각을 했다. 그 이유는 큰 데이터가 아니라면 딥러닝을 안써도 되고, 더 좋은 방법이 많다고 생각하기 때문이다. 내가 생각하는 scalable 은 뭐 다른 부분이야 결과가 잘 나오면 되는 것이겠으나, 연산량에 있어서 O(n) 을 넘어서는 안된다는 것이다. O(n) 을 넘어서면 데이터가 커지면 커질수록 연산량이 너무 많아질 것이기 때문이다.

이상 느낀 점 끝
