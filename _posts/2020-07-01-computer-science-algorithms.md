---
layout: post
topic: computer-science
title: 알고리즘
---

자료구조를 학습한 후 알고리즘을 공부하리라 다짐했었다.  
6 월에 자료구조를 마친 후 이제 알고리즘을 학습해보려고 한다.  
알고리즘 역시 참고할 수 있는 자료가 많고 그 컨셉이 다양한데,  
각 책마다 문제풀이에 집중하거나 이론에 집중하는 등 그 컨셉이 다르다.  


여자친구에게 의견을 구해보니 일단 이론적인 것을 학습한 후  
문제풀이는 리트코드 등의 온라인 사이트를 이용하는 것이 좋을 것 같다 하여 그렇게 하기로 했다.  

읽을 책은 한빛 교재 시리즈 중 하나인 "쉽게 배우는 알고리즘 - 관계 중심의 사고법" 이라는 책이다.  
목차를 대충대충 살펴보니 자료구조와 겹치는 부분이 많았다.  
사실 상 자료구조의 심화버젼이라고 해도 될 정도였다.  

정리할 내용이 많을 듯 싶다. 운영체제 정리와 다르게 그림, 의사코드 및 수식 등이 많이 등장할 듯 하다.  
책에서 소개하는 알고리즘에 대해 실제 코드를 구현해보면 좋겠지만 그렇게 하면 너무 오래 걸릴 듯 하다..  
그러므로 운영체제와 마찬가지로 챕터 별로 중요한 내용만 추려서 정리할 것이다.  

---

### 1. 알고리즘 설계와 분석의 기초

- 알고리즘의 중요성과 표기법에 대해 다루는 챕터
- 표기법 및 그 뜻은 아래와 같다.

$$
\text{1.} \; O(f(x)) = \{ g : \lim_{n \rightarrow \infty} \frac{g(x)}{f(x)} < \infty \} \quad \text{(점근적 상한)} \\
\text{2.} \; \Omega(f(x)) =  \{ g : \lim_{n \rightarrow \infty} \frac{f(x)}{g(x)} < \infty \} \quad \text{(점근적 하한)} \\
\text{3.} \; \Theta(f(x)) = O(f(x)) \cap \Omega(f(x)) \\
\text{4.} \; o(f(x)) = \{ g : \lim_{n \rightarrow \infty} \frac{g(x)}{f(x)} = 0 \} \quad \text{(여유있는 상한)} \\
\text{5.} \; \omega(f(x)) =  \{ g : \lim_{n \rightarrow \infty} \frac{f(x)}{g(x)} = 0 \} \quad \text{(여유있는 하한)} \\
$$

- 여담으로 보통 빅-오를 세타로 표기하는 경우가 많은 듯..

---

### 2. 점화식과 점근적 복잡도 분석

- 점화식으로 표현된 시간 복잡도를 단일식으로 표현하는 방법을 배운다.  
- 반복 대치, 추정 후 증명, 마스터 정리 세 가지 방법이 있다.  
- 반복 대치는 식을 반복하여 계산하는 방법이다. 예를 들어 T(n) <= T(n/2) + n 이라고 하면 
T(n) <= T(n/2) + n <= n + n/2 + T(n/4) <= ... = O(n * log2 n]) 이 성립한다.  
- 추정 후 증명은 말 그대로 추정 후 증명하는 방법이다. 수학적 귀납법 등이 증명에 사용된다.  
- 마스터 정리는 T(n) = aT(n/b) + f(n) 의 형식으로 표현되는 재귀식의 복잡도를 알 수 있는 정리이다.  
- n^(logb a) = h(n) 이라고 하면
- h(n) 이 더 복잡하면 h(n) 이, f(n) 이 더 복잡하면 f(n) 이 수행시간을, 똑같으면 logn * h(n) (또는 동일하게 f(n)) 이 수행시간을 지배한다는 정리이다.

---

### 3. 정렬

- 선택정렬의 의사코드는 아래와 같다.
```
1. 배열의 길이가 n 인 배열에서 최대값을 찾는다.  
2. 배열의 마지막 위치에 있는 값과 최대값의 위치를 바꾼다.    
3. 배열의 길이가 n - 1 이라고 가정하여 위 가정을 반복한다.  
```

- 버블정렬의 의사코드는 아래와 같다.
```
1. 첫 번째와 두 번째 원소를 비교하여 첫 번째 원소가 크면 위치를 변경한다.  
2. 두 번째와 세 번째 원소를 비교하여 두 번째 원소가 크면 위치를 변경한다.  
...
위 과정을 거치면 제일 마지막에 최대값이 위치하게 된다.  
위 과정을 배열의 길이가 n 일 때 했다고 가정하면 이후 배열의 길이가 n - 1 이라 가정하여 위 가정을 반복한다.
```

- 삽입정렬의 의사코드는 아래와 같다.
```
첫 번째 원소는 무조건 정렬되어 있다.
두 번째 원소를 비교하여 정렬한다.
세 번째 원소와 위 두 개 값이 정렬된 배열을 비교하여 정렬한다.
네 번째 원소와 위 세 개 값이 정렬된 배열을 비교하여 정렬한다.
...
위 과정을 반복한다.
```

- 병합정렬의 의사코드는 아래와 같다.
```
배열을 위치를 기준으로 반으로 나눈다.
왼쪽 배열을 병합정렬을 사용하여 정렬한다.
오른쪽 배열을 병합정렬을 사용하여 정렬한다.
왼쪽 배열과 오른쪽 배열을 삽입 정렬을 응용하여 정렬한다.
```

- 퀵정렬의 의사코드는 아래와 같다.
```
1. 배열에 있는 임의의 값을 기준으로 왼쪽에 위치한 원소는 기준값보다 작도록,  
오른쪽에 위치한 값은 기준값보다 크도록 불완전하게 정렬한다.
2. 왼쪽 배열을 1. 의 과정을 반복하여 완벽한 정렬을 만든다.  
3. 오른쪽 배열을 1. 의 과정을 반복하여 완벽한 정렬을 만든다.
```

- 힙정렬의 의사코드는 아래와 같다.
```
1. 배열의 원소를 차례대로 힙에 삽입한다.  
2. 힙에서 차례대로 원소를 빼내어 배열에 채운다.  
(힙은 특별한 트리 중 하나로 루트 노드에 High Priority 를 갖는 원소가 위치한다.)
```

- 결정트리를 활용하면 두 원소 간의 비교가 필요한 정렬은 최소 O(n*log n) 의 시간복잡도가 필요함을 증명할 수 있다.  

- 기수정렬의 의사코드는 아래와 같다.  
```
1. 각 원소의 길이가 같고 그 가능성이 제한된 배열이 존재할 때 (ex. 네자리 수만으로 배열이 구성되어 있을 때)
2. 가장 적은 영향을 미치는 원소 내 위치를 기준으로 bucket 를 만들어 정렬한다. (ex. 1의 자리수로 배열한다.)
3. 차례대로 큰 영향을 미치는 원소 내 위치를 기준으로 bucket 을 만들어 정렬을 수행한다.
```

- 계수정렬의 의사코드는 아래와 같다.
```
1. 각 원소의 존재 가능한 값이 제한되어 있을 때  
2. counter 를 만들어서 배열을 순회하여 원소의 개수를 구한다.  
3. counter 에서 차례대로 빼냄으로써 정렬을 수행한다.
```

- 위 두 정렬은 원소의 형태에 제한을 함으로써 O(n) 의 성능을 낼 수 있는 특수한 정렬 중 하나이다.  

---

### 4. 선택 알고리즘

- k 번 째로 작은 원소를 탐색하기 위해 퀵 정렬을 응용하여 임의의 기준값을 바탕으로 배열의 원소를 줄여가면서 탐색할 수 있다.  
- 최악의 경우에 O(n^2) 이 소요되나 평균적으로 O(n) 이 소요된다.  
- linear select 를 사용하면 최악의 경우에도 선형시간이 보장되지만 오버헤드가 클 수 있다.  
- linear select 가 선형시간이 보장되는 이유는 M 을 기준으로  
왼쪽 배열과 오른쪽 배열의 비율이 차이가 나봤자 최대 1:3 이라는 것이 보장되기 때문이다.
```
Linear Select
1. 원소의 총 수가 5개 이하이면 원하는 원소를 찾고 알고리즘을 끝낸다.  
2. 배열을 배열의 길이가 5개로 이루어진 sub 배열들로 나누어 각 배열의 중앙값을 구한다.  
3. 중앙값들의 중앙값을 Linear Select 를 호출하여 구하고 이를 M 이라고 한다.
4. M 을 기준으로 퀵정렬과 유사하게 배열을 분할하고, 적합합 그룹을 기준으로 Linear Select 를 호출한다.
```
---

### 5. 검색트리

- 다양한 검색트리에 대해서 배우는 챕터이다.
- 각 트리마다의 특성과 삽입, 삭제하는 의사코드를 다 적기에는 어려우므로 그 컨셉만 기술하도록 하겠다.


- 이진검색트리란 검색트리의 기본이 되는 트리로 아래와 같은 특성을 갖는다.
```
1) 최상위 루트 노드가 있고, 각 노드는 최대 두 개의 자식 노드를 갖는다.  
2) 임의의 노드의 키 값은 자신의 왼쪽 서브트리의 임의의 키값보다 크고, 오른쪽 서브트리의 임의의 키값보다 작다.
3) 왼쪽 서브트리와 오른쪽 서브트리도 이진검색트리의 특성을 만족한다.
```
- 이진검색트리는 최선의 경우에 O(log n) 의 성능을 갖게 되나,  
삽입 및 삭제 순서에 따라 노드가 일렬로 배치되어 O(n) 의 성능을 갖게될 가능성이 존재한다.


- 레드블랙트리는 이진트리를 응용한 트리로 아래와 같은 특성을 갖는다.
```
1) 각 노드는 블랙 또는 레드의 색상을 갖는다.
2) 루트는 블랙이다.
3) 모든 리프(NIL) 은 블랙이다.
4) 노드가 레드이면 그 노드의 자식은 반드시 블랙이다.
5) 루트 노드에서 임의의 리프 노드에 이르는 경로에서 만나는 블랙 노드의 수는 모두 같다.
** 레드블랙트리에서의 리프 노드는 다른 트리에서와 다르게 NIL 로 가정한다.
```
![](/assets/img/computer_science_algorithms/red-black-tree.png)


- 레드노드가 연이어 나올 수 없고, 리프노드까지 갈 때에 마주치는 블랙노드 수가 같다는 특성 때문에 레드블랙트리의 높이는 완전이진트리의 높이의 두 배를 넘지 못한다.  
- 삽입할 때에 이진트리와 같은 방법으로 삽입한 후 삽입 노드에 레드 색상을 부여한다.  
레드 색상을 부여하면 임의의 리프 노드까지 이르는 경로에서 마주치는 블랙 노드의 수가 같다는 조건을 충족한다.  
한편 부모노드가 레드 색상이면 조건을 위반하게 되어 이 문제를 해결하기 위해 부모의 부모 노드까지 고려하여 이 문제를 해결한다.  
- 삭제할 때에 이진트리와 같은 방법으로 리프 노드와 해당 노드의 값을 바꾼 후 리프 노드를 삭제한다.  
삭제하는 리프 노드가 레드 색상이면 조건을 모두 충족시키므로 문제가 없지만,  
블랙 노드라면 임의의 리프 노드까지 이르는 경로에서 마주치는 블랙 노드의 수가 같다는 조건이 위반되어 이를 해결하기 위해 이 문제를 해결하기 위해 부모의 부모 노드까지 고려하여 이 문제를 해결한다.  


- B Tree 는 디스크에 저장된 정보에 대하여 접근할 때 주로 사용되는 트리이다.
- 디스크에 접근하는 알고리즘에서 그 성능은 디스크 접근 횟수에 크게 좌우되므로 접근 횟수를 줄이는 것이 성능에 좋은 영향을 미친다.
- DB Index 등에서 응용된다.  
- B Tree 는 균형잡힌 다진검색트리로 다음의 성질을 만족한다.  
```
1) 루트를 제외한 모든 노드는 [k/2] ~ k 개의 키를 갖는다.
2) 모든 리프 노드는 같은 깊이를 갖는다.
```
- 다른 트리와 다르게 아래처럼 키 값을 경계점으로 하여 서브트리가 연결되어 있다.  
![](/assets/img/computer_science_algorithms/B-tree.png)


- 삽입할 때에 해당 위치를 찾고 그 공간에 여유가 있으면 삽입하며 완료된다.  
- 공간에 여유가 없으면 형제 노드를 살펴보고 형제 노드에 키를 넘기며 완료된다.  
- 형제 노드에 여유가 없으면 노드를 두 개로 분리한다. 이러한 오버플로우는 부모 노드에서 발생할 수 있는데 이 역시 재귀적으로 해결한다.  
- 삭제할 때에 이진검색트리와 마찬가지로 삭제 노드를 찾아 리프 노드와 값을 바꾼 후 리프 노드를 삭제한다.  
- 삭제 후 언더플로우가 생기면 병합 과정을 거쳐 해결한다.


- 아래부터는 복수 개의 키 값을 갖는 레코드로 구성되는 트리이다.  


- KD-트리란 이진 검색트리를 확장한 것으로 k개의 필드로 이루어진 키를 사용한다.
- 각 레벨을 하나의 차원만을 다루고, 필드 별로 순회하며 트리를 구성하게 된다.  
- 예를 들어 레코드가 (x, y, z) 의 3차원으로 구성되어 있다면 첫 번째에서는 x 를 기준으로 분기하고, 두 번째에서는 y 를, 세 번째에서는 z 를, 네 번째에서는 다시 x 를 사용하여 분기를 수행한다.  
- 검색과 삽입은 이진검색트리를 단순히 확장한 것이라고 볼 수 있다.
- 반면 삭제할 때에는 각 레별 별로 분기의 기준이 되는 필드가 다르므로 이를 적절히 고려하여 분기를 재조정해야 한다.
- 반면 특정 필드의 최소값을 찾는 문제는 이진트리보다 복잡하다. 왜냐하면 각 레벨 별로 분기에 사용되는 필드가 달라 다수의 경로를 탐색해야 하기 때문이다.


- KDB-트리란 다차원 검색을 다룰 수 있도록 B Tree 를 확장한 것이다. 기본적으로 B Tree 를 확장한 것이므로 B tree 의 성질을 충족한다.
- B Tree 는 각 노드가 키 값에 의해 분기하지만 KDB 트리는 각 노드가 영역에 의해서 분기한다.  
- KDB 트리의 노드는 다음의 두 종류가 있다.  
```
영역 노드: 복수 개의 (영역, 페이지 번호) 쌍으로 구성된다. 모든 내부 노드는 영역 노드이다.
키 노드: 복수 개의 (키, 페이지 번호) 쌍으로 구성된다. 모든 리프 노드는 키 노드이다.
```
- k 차원의 키를 사용한다면 영역은 (<min1 , max1 >, <min2 , max2 >, ..., <min k, max k>) 처럼 나뉜다.
- 영역 노드 간에 영역이 겹치는 경우는 없다.
- KDB 트리에서의 키 검색은 루트 노드부터 시작해서 해당 키가 포함되는 영역을 따라 리프 노드까지 내려가면 되므로 간단하다.
- KDB 트리에서는 영역으로 노드를 나누므로 영역 검색도 가능하다.
- 삽입과 삭제는 B-Tree 와 유사하다. 해당 키가 포함되는 영역에 키를 삽입한 후 오버플로우가 발생하면 영역을 적절히 분할한다.
- 내부 노드는 모두 영역 노드이므로 직접적인 삭제가 발생할 수 없다. 즉 삭제는 리프노드에서 발생하고, 언더플로우가 발생하면 영역을 적절히 병합한다.


- R-트리란 KDB-Tree 와 마찬가지로 다차원 검색을 다룰 수 있도록 B-Tree 를 확장한 것이다. 기본적으로 B Tree 를 확장한 것이므로 B tree 의 성질을 충족한다.
- KDB 트리와 다른 점은 KDB 트리는 영역 노드를 구성할 때에 전체 영역(각 필드별 최소값과 최대값) 을 기준으로 영역을 분할하는데,  
R 트리는 레코드의 키 값을 고려하여 영역을 구성한다. 예를 들어 키가 (x, y) 일 때에 x 와 y 가 거의 일직선 상에 놓여있다면 반대 일직선 상에 위치한 영역에 대해서는 해당하는 영역 노드를 구성하지 않는다.  
R 트리를 구성하는 영역은 KDB 트리와 다르게 disjoint 하지 않아 검색 경로가 유일하지 않을 수 있다.  
- 이러한 문제를 개선하여 각 영역이 겹치지 않도록 R Tree 를 확장한 것이 R* Tree 이다.
- 삽입과 삭제는 B Tree, KDB Tree 와 유사한 컨셉으로 진행된다.


- 검색트리는 아니지만 유사한 컨셉을 갖는 그리드 파일이라는 개념이 존재하여 이를 소개하도록 한다.
- 그리드파일은 KDB 트리와 마찬가지로 각 영역을 분할하여 레코드를 저장한다.
- 각 영역은 경계점을 기준으로 구분되며 삽입 및 삭제 시 적절히 영역을 분할 및 병합한다.
- 각 영역은 그 크기에 따라 같은 페이지에 저장되어 있을 수도 있다.
- 데이터 접근을 위해 일차배열 스케일링에 대한 접근과 해당 페이지로의 접근만 하면 되므로 두 번의 디스크 접근으로 데이터에 접근할 수 있다는 장점이 있다.

---

### 6. 해시 테이블

- 해시 테이블은 원소끼리 비교해 자리를 찾는 것이 아니라 자신의 값이 저장되는 자리를 바로 결정한다.
- 해시 테이블에 원소가 차 있는 비율을 적재율(Load Factor) 라고 하고, 이는 해시 테이블의 성능에 중요한 영향을 미친다.
- 두 원소의 해시값이 동일한 경우 같은 자리에 삽입하려고 하는 상황이 발생하고, 이를 충돌(Collision) 이라고 한다.
- 해시 함수는 입력 원소가 테이블 전체에 균등하게 저장되도록 해시 값을 산출해야 하며, 계산이 간단해야 한다.
- 해시 함수를 만드는 대표적인 두 가지 방법은 나누기와 곱하기이다.  

```
m 은 해시 테이블의 크기이다.

1) 나누기 방법
- h(x) = x mod m
- m 은 2의 멱수에 가깝지 않은 소수를 택하는 것이 좋다. 
- 만일 m = 2^p 라면 하위 p 개 비트에 의해 해시값이 결정되어 분산시키기 어렵기 때문이다.

2) 곱하기 방법
- h(x) = [m (xA mod 1)]
- m 은 해시 테이블의 크기이고, A 는 해시 함수의 특성을 결정짓는 상수이다.
- 나누기 방법과 다르게 m 의 선택에 제한이 없다. 그러나 컴퓨터 환경에 맞게 보통 m = 2^p 로 잡는다.
```


- 체이닝 방법은 같은 주소로 해싱되는 원소를 하나의 연결 리스트에 매달아 관리하는 방법이다.
- 효율성 측면에서 새로 삽입되는 원소는 연결리스트에 제일 앞에 위치한다.
- Load Factor 가 1 을 초과하여도 사용할 수 있다.


- 개방주소(Open Addressing) 방법은 체이닝과 같은 추가 공간을 허용하지 않는다.
- 체이닝 방법과 다르게 Load Factor 가 1을 초과할 수 없다.
- 해시 함수를 0 번째 해시 함수, 충돌이 한 번 일어났을 때 다음 주소를 계산하는 것을 두 번째 해시 함수, ... 와 같이 여러 개의 해시 함수를 사용한다.
- 다음 주소를 결정짓는 대표적인 세 가지 방법은 선형조사, 이차조사, 더블 해싱이다.
- 개방주소 방식에서 특정 원소를 삭제한 후 검색을 하게 되면 해당 원소가 존재하더라도 그 경로 상에 데이터가 없어서 탐색되지 않는 문제가 발생할 수 있다.
- 따라서 삭제한 후에 DELETED 라는 상수값으로 대체하여 이후 충돌 해결 해시 값에 위치한 원소에 대하여도 탐색할 수 있도록 해야 한다.

```
선형조사 방법
h_i(x) = h(x) + i mod m
- 간단한 충돌 해결 방법으로 특정 영역에 원소가 몰릴 때에 치명적으로 성능이 떨어진다.
- 이러한 현상을 1차 군집(Primary Clustering) 이라고 한다.
```


```
이차조사 방법
h_i(x) = h(x) + a*i^2 + b*i mod m
- 해시 값이 특정 영역에 집중되어도 해당 영역을 선형조사 방법보다 빠르게 벗어날 수 있다는 장점이 있다.
- 그러나 여러 개의 원소가 동일한 초기 해시 값을 갖게 되면 똑같은 경로를 탐색하게 되므로 2차 군집(Secondary Clustering) 이 발생한다.
```


```
더블해싱
h_i(x) = h(x) + if(x) mod m
- 서로 다른 두 해시 함수를 갖고 해시값을 계산한다. 1차 군집과 2차 군집 현상이 발생하지 않는다.
- 권장되는 방법은 h(x) = x mod m 으로 잡고 f(x) = 1 + (x mod m') 으로 정하는 것이다. 이 때 m' 은 m 보다 조금 작은 소수이다.
```

---

### 7. 상호배타적 집합의 처리

상호배타적 집합의 관리를 위해 아래 세 가지 연산이 필요하다.

1. make set(x): 원소 x 로 이루어진 집합을 만든다.
2. find set(x): 원소 x 가 포함된 집합을 찾는다.
3. union(x, y): 원소 x 를 가진 집합과 원소 y 를 가진 집합을 하나로 합친다.

위 상호배타적 집합을 구현하기 위해 아래처럼 연결리스트를 활용할 수 있다.  
일반적인 이중연결리스트와 다른 점은 head 가 대표 원소를 가리킨다는 점이다.  
![](/assets/img/computer_science_algorithms/disjoint_set_linked_list.png)  

- make set, find set 의 수행 시간은 O(1) 인 반면, Union 은 특정 set 의 head 를 모두 바꿔야 하므로 O(1) 을 초과한다.
- 두 집합을 union 할 때 원소의 개수가 적은 집합의 head 를 변경하는 것이 비용이 적다. 이렇게 개수를 고려하여 union 하는 것을 **Weighted Union** 이라고 한다.
- 아래와 같은 정리가 성립한다.

```
정리) 연결 리스트를 이용해 표현되는 배타적 집합에서 Weighted Union 을 사용할 경우,
m 번의 make set, find set, union 연산 중 make set 연산이 n 번 발생한다면 이들의 총 수행시간은
O(m + n*log n) 이다.

증명) make set 과 find set 은 O(1) 이므로 이 둘에 의한 연산량은 O(m) 이다.
make set 연산이 n 번 발생하였으므로 원소의 개수는 n 개이고,
Weighted Union 을 사용하므로 모든 원소가 union 되었다는 가정 하에 최대의 연산량을 역으로 계산하면
O(n*log n) 임을 알 수 있다.
따라서 O(m + n*log n) 이 성립한다.
```

위 상호배타적 집합을 구현하기 위해 아래처럼 트리 구조로 집합을 표현할 수도 있다.  
일반적인 트리와 다른 점은 자식 노드를 갖는 것이 아니라 부모 노드를 갖는다는 것이다.  
![](/assets/img/computer_science_algorithms/disjoint_set_tree.png)  

- make set 은 O(1) 의 연산량이 필요하다.
- union 을 효율적으로 하기 위해 tree 의 rank 를 활용할 수 있다. rank(= depth) 가 낮은 트리가 높은 트리의 자식 노드로 귀속한다. 이를 **Rank Union** 이라고 한다.
- find set 을 할 때 **경로 압축** 을 사용할 수 있다. 경로 압축이란 루트 노드로 향하는 과정에서 거쳤던 노드들을 모두 루트 노드의 자식 노드로 귀속시키는 방법이다.
- 위 과정을 따라 union 과 find set 을 구현하면 아래와 같은 정리를 만족한다.  

```
정리1) rank 를 이용한 union 을 하면 랭크가 k 인 노드를 대표로 하는 집합의 원소 수는 최소 2^k 개이다.

정리2) rank 를 이용한 union 을 하면 원소 수가 n 인 집합에서 임의의 노드의 랭크는 O(log n) 이다.

정리3) 트리를 이용해 표현되는 배타적 집합에서 Rank Union 을 사용할 경우,
m 번의 make set, find set, union 연산 중 make set 연산이 n 번 발생한다면 이들의 총 수행시간은
O(m * log n) 이다.

정리4) 트리를 이용해 표현되는 배타적 집합에서 Rank Union 과 경로압축을 사용한다면,
m 번의 make set, find set, union 연산 중 make set 연산이 n 번 발생한다면 이들의 총 수행시간은
O(m log^* n) 이다.

** log^* n 이란 f(x) = log(x) 라고 했을 때 f^k(x) < 1 이 성립하는 최초의 k 를 말한다.
이는 매우 효율적인 것으로 log^* 2^65536 = 5 가 성립할 정도이다.
```

---

# 8. 동적 프로그래밍

- 동적 프로그래밍은 재귀 호출로 주어진 문제를 풀었을 때, 지나친 중복이 발생할 때 재귀적 중복을 해결하는 방법을 말한다.
- 예를 들어 피보나치 수를 구하는 함수를 재귀적으로 정의했을 때 중복된 호출이 발생하고, O(2^n) 연산량이 발생한다.
- 배열을 사용하여 계산된 피보나치 값을 저장하고 반환한다면 O(n) 으로 성능이 향상된다.
- 위와 같이 문제 풀이에서 경유한 값을 저장하여 사용하는 것을 Memoization 이라고 한다.

<br>

**행렬 경로 문제**
  
- 행렬 경로 문제란 주어진 행렬의 좌측상단에서 우측하단으로 이동할 때 가장 적은 비용을 계산하는 것이다.
- 제한: 진행 방향은 오른쪽이나 아래쪽으로 제한된다.  

![](/assets/img/computer_science_algorithms/shortest_path_matrix.png)

```
m_ij 가 (i, j) 위치의 비용이고, c_ij 가 좌측 상단에서 해당 좌표까지 이동할 때에 발생하는 가장 적은 비용이라면
아래 식이 성립힌다.

c_ij
= m11 (if i=j=1)
= m1j + c1(j-1) (if i=1, j>1)
= mi1 + c(i-1)1 (if i>1, j=1)
= mij + max { ci(j-1), c(i-1)j } (if i>1, j>1)
```

- 위를 재귀적으로 호출할 때에 Memoization 을 수행하지 않으면 중복된 계산이 매우 많이 수행된다.
- Array\[m\]\[n\] 의 형태로 c_ij 를 Memoization 하여 비효율성을 해결할 수 있다.

<br>

**조약돌 놓기 문제**

- 조약돌 놓기 문제란 3 x n 테이블의 각 칸에 숫자가 적혀있을 때
- 아래 조건을 만족시키며 조약돌을 놓았을 때 조약돌이 놓인 위치의 합을 최대로 하는 문제이다.
- 제한1: 가로나 세로로 인접한 두 칸에 동시에 조약돌을 놓을 수 없다.
- 제한2: 각 열에 하나 이상의 조약돌을 놓는다.

![](/assets/img/computer_science_algorithms/putting_stone.jpeg)

```
한 열에 대하여 위 그림처럼 네 가지 패턴이 가능하다. 오른쪽에 이어진 그림은 다음 열에서 사용 가능한 패턴이다.
i 열에서 주어진 패턴으로 놓았을 때 얻을 수 있는 점수를 w_ip,
i 열에서 주어진 패턴으로 놓았을 때 1열부터 i열까지의 점수의 합 중 가장 큰 값을 c_ip 라고 할 때,
아래 등식이 성립한다.

c_ip
= w_ip (if i=1)
= max {c_(i-1)q} + w_ip (if i>1, q 는 i-1 열에서 사용 가능한 패턴)
```

- Array\[p\]\[n\] 의 형태로 Memoization 하여 효율을 높일 수 있다. \(p 는 가능한 패턴의 개수\)


<br>

**행렬 곱셉 문제**

- n 개의 행렬 A1, A2, ... An 이 주어지고 이들의 곱 A1\*A2\*A3...An 을 계산하려 한다.
- 행렬 곱셈 문제는 어떠한 순서로 곱셈을 진행해야 최소한의 연산으로 곱셈을 마칠 수 있는지 결정하는 문제이다.
- p x q, q x r 의 행렬의 곱을 계산할 때 필요한 연산량은 pqr 이다.

```
A1, ..., An 의 형태를 p0 x p1, p1 x p2, ... p(n-1) x pn 이라고 하고,
c_ij 를 행렬 Ai * A(i+1) * ... Aj 를 계산하는 최소 비용이라고 하면 다음이 성립한다.

c_ij
= 0 (if i=j)
= min { c_ik + c(k+1)j + p(i-1)*pk*pj } (if i<j)
```

- Array\[n\]\[n\] 의 형태로 c_ij 를 Memoization 하여 효율을 높일 수 있다.


<br>


**최장 공통 부분 순서(LCS, Longest Common Subsequence)**

- 두 문자열이 주어졌을 때 공통적으로 존재하는 가장 큰 부분순서를 구하는 문제이다.
- 부분 순서는 원 문자열에서 연속될 필요가 없다.
- 예를 들어 \<bcdb\> 는 \<a**bc**b**d**a**b**\> 의 부분순서이다.

```
두 문자열을 X_m = <x1x2...xm>, Yn = <y1y2...yn> 이라고 하고,
c_ij 를 X' = <x1...xi>, Y' = <y1...yj> 의 LCS 길이라고 하면 다음이 성립한다.

c_ij
= 0 (if i=j=0)
= c_(i-1)(j-1) + 1 (if xi=yj)
= max { c_(i-1)j, c_i(j-1) } (if xi != yj)
```

- Array\[m\]\[n\] 의 형태로 Memoization 하여 효율을 높일 수 있다.

---

# 9. 그래프 알고리즘

- 그래프는 사물을 정점과 간선으로 표현하는 것으로 정점(Vertex)은 대상이나 개체를 나타내고 간선(Edge)은 이들간의 관계를 나타낸다.
- 간선이 방향을 갖고 있다면 유향 그래프(Directed Graph), 방향이 없다면 무향 그래프(Undirected Graph) 또는 무방향 그래프라고 한다.
- 또한 간선은 가중치를 가질 수 있다. 도시 간의 거리 등이 대표적인 간선의 예이다.
- 그래프 G 는 정점을 V, 간선을 E 라고 하면 G = (V, E) 라고 한다.


**그래프의 표현**

- 인접행렬을 이용하여 그래프를 표현하면 n^2 의 공간이 필요하다. 간선의 밀도가 아주 높은 그래프에서 적합하다.
- 인접리스트를 이용하여 그래프를 표현하면 공간의 낭비가 없으나 오버헤드가 존재한다. 간선의 밀도가 낮은 그래프에 적합하다.


**너비우선탐색(BFS) 와 깊이우선탐색(DFS)**

- 너비우선탐색은 주어진 정점에서 인접한 순서대로 정점을 이동하며 탐색한다. Queue 로 구현된다.
- 너비우선탐색을 할 때에 경유한 간선의 그래프는 Cycle 이 없으므로 트리를 구성한다. 이를 너비우선트리라고 한다.
- 깊이우선탐색은 주어진 정점에서 도달할 수 있는 가장 깊은 정점까지 차례대로 탐색한다. Stack 또는 재귀로 구현된다.
- 깊이우선탐색을 할 때에 경유한 간선의 그래프는 Cycle 이 없으므로 트리를 구성한다. 이를 깊이우선트리라고 한다.


**최소신장트리**

- 프림 알고리즘: 프림 알고리즘은 집합 S 를 공집합에서 시작하여 모든 정점을 포함할 때까지 확장한다.
- 확장은 집합 S 에 포함되지 않은 정점 중 가장 비용이 적은 정점을 추가하는 식으로 진행된다.
- 프림 알고리즘의 수행시간은 O(E log V) 이다.


- 크루스칼 알고리즘: 크루스칼 알고리즘은 간선 E 를 비용 순으로 정렬하고, Cycle 이 생성되지 않도록 차례대로 추가하는 형식으로 수행된다.
- 마찬가지로 O(E log V) 의 시간이 소요된다.


- **안전성 정리**란 프림 알고리즘과 크루스칼 알고리즘의 수학적 토대가 된다.

```
안전성 정리
: 간선이 가중치를 갖는 무방향 그래프 G =(V, E) 의 정점들이 임의의 두 집합 S 와 V-S 로 나누어져 있다고 가정하자.
간선 (u, v) 가 S 와 V-S 사이에 존재하는 최소 교차간선이라면 (u, v) 를 포함하는 최소신장트리가 존재한다.
```


**위상정렬**

- 정점이 부분적으로 ordering 될 수 있을 때 위상정렬을 할 수 있다.
- 예를 들어 라면을 끓이기 위해 물을 끓이는 것과 라면봉지를 뜯는 것은 그 순서에 상관이 없으나 라면을 냄비에 넣는 것은 두 개 행동 모두가 수행되고 난 후에 가능하다.
- 따라서 부분적으로 (물 끓이기, 라면 봉지 뜯기) < 라면 냄비에 넣기의 관계가 성립한다.
- 이러한 부분 순서를 갖고 있는 그래프를 정렬하는 것이 위상정렬이고, 그 의미 상 방향 그래프여야 한다.
- 진입간선이란 특정 정점으로 향하는 간선을 뜻하고, 진출간선이란 특정 정점에서 시작하는 간선을 말한다.
- 위상 정렬은 아래처럼 구현되는데, 진입간선이 없는 정점을 효율적으로 탐색하기 위해 적절히 구현되어야 한다.

```
G = (V, E) 의 정점의 개수가 n 이라고 할 때,

A := Vertex[n]
for i <- 1...n
    u <- 진입간선이 없는 정점
    정점 u 와 진출간선을 모두 제거
    A[i] = u

라고 하면 A 는 G 가 위상정렬되어 있는 배열이다.
```

- DFS 를 활용한다면 아래처럼 구현할 수도 있다.
- DFS 를 활용하여 구현할 때에는 아무 정점으로부터 시작해도 상관없다.
- 해당 정점으로부터 탐색되지 않은 깊이까지 탐색하고, 해당 정점을 리스트에 삽입한다.
- 가장 깊이 탐색되었다는 것은 visited 배열을 활용하여 확인할 수 있다.


- 두 알고리즘 모두 O(E + V) 의 시간이 소요된다.


**최단경로**

- 그래프 G 가 가중치를 갖는 유향 그래프라면 최단경로 문제가 성립힌다.
- 다익스트라 알고리즘, 벨만-포드 알고리즘, 모든쌍 최단경로 알고리즘, 싸이클이 없는 그래프의 최단경로 알고리즘 등이 최단경로 문제를 풀기 위해 사용된다.
- 최단경로 문제에서 그래프의 유형은 모든 간선 가중치가 음이 아닌 경우와 음의 가중치를 가진 간선이 존재하지만 임의의 Cycle 을 구성하는 간선 가중치의 합은 음이 아닌 경우로 나뉜다.
- 다익스트라 알고리즘, 벨만-포드 알고리즘, Cycle 이 없는 그래프의 최단경로 알고리즘은 하나의 시작 정점으로부터 다른 모든 정점까지의 최단경로를 구한다.


- 다익스트라 알고리즘(음의 가중치를 허용하지 않는 경우)은 아래와 같다.
- 다익스트라 알고리즘의 수행시간은 O(E logV) 이다.

```
Dijkstra(G, r)
> G = (V, E) 로 음의 가중치가 없는 유향 그래프
> r: 시작 정점

d: r 에서부터의 거리를 기록할 자료구조
prev: 어떤 정점을 경유하는지 기록할 자료구조 
S <- empty

for each u in V:
    d[u] = inf
d[r] = 0

while S != V
    u <- V - S 중 거리(d) 가 가장 작은 정점
    S = S union {u}
    for each v in L(u) (= u로부터 연결된 정점의 집합)
        if v in V - S and d[u] + w(u, v) < d[v]
            d[v] = d[u] + w(u, v)
            prev[v] = u
```

- 벨만-포드 알고리즘(음의 가중치를 허용하나 임의의 Cycle 의 가중치의 합은 음이 아닌 경우)은 아래와 같다.
- 벨만-포드 알고리즘의 수행시간은 O(EV) 이다.

```
BellmanFord(G, r)
> G = (V, E) 로 음의 가중치가 존재하지만 임의의 Cycle 의 가중치 합은 음이 아닌 경우
> r: 시작 정점

d: r 에서부터의 거리를 기록할 자료구조
prev: 어떤 정점을 경유하는지 기록할 자료구조
S <- empty

for each u in V:
    d[u] = inf
d[r] = 0

# 최단경로 이므로 임의의 정점으로의 이동이라 하더라도 최대 |V| - 1 개의 간선을 경우하기 때문
# 만약 최대 k 개의 간선을 경유하는 최단 경로를 구하고 싶다면 k 번 순회하면 됨
for i in 1...|V| - 1
    for each (u, v) in E
        if d(u) + w(u, v) < d[v]
            d[v] = d[u] + w[u, v]
            prev[v] = u

# 음의 Cycle 이 존재하는지 확인
for each (u, v) in E
    if d[u] + w(u, v) < d(v)
        then Error
```

- 모든 쌍 최단경로 알고리즘의 간단한 구현은 아래와 같다.
- 소요시간은 O(V^4) 이다.

```
d_ij^m : 최대 m 개의 간선을 이용해 i 에서 j 까지 이르는 최단거리

d_ij^m
= w_ij (if m = 1)
= min_{1 <= k <= n} {d_ik^m-1 + w_kj} (if m > 1)

따라서 두 정점 사이에 간선이 존재하지 않는 경우 가중치를 inf,
같은 정점인 경우의 가중치를 0 이라고 가정하면 아래처럼 계산할 수 있다.

for i in V
    for j in V
        d_ij^1 = w_ij

for m in 2...|V| - 1:
    for i in V
        for j in V
            d_ij^m = min_{1 <= k <= n} {d_ik^m-1 + w_kj} (if m > 1)
```

- 모든 쌍 최단경로를 구하기 위해 플로이드-워샬 알고리즘을 사용할 수 있다.
- 플로이드-워샬 알고리즘의 수행시간은 O(V^3) 이다.

```
d_ij^k: 정점 집합 { 1, 2, ... k } (= S) 에 속하는 정점들만을 중간 정점으로 거쳐서 i 에서 j 에 이르는 최단거리

d_ij^k
= w_ij (if k=0)
= min{ d_ij^k-1, d_ik^k-1 + d_kj^k-1 }

:FloydWarshall(G)

for i in 1...n
    for j in 1...n
        d_ij^0 = w_ij

for k in 1...n
    for i in 1...n
        for j in 1...n
            d_ij^k = min{ d_ij^k-1, d_ik^k-1 + d_kj^k-1 }
```

- 싸이클이 없는 유향 그래프(DAG, Directed Acyclic Graph) 은 위상정렬을 응용하여 최단경로를 구할 수 있다.
- 위상정렬을 하는 데에 드는 비용이 O(V + E) 이고 dominant 하므로 DAG shortcut 의 소요시간은 O(V + E) 이다.
- DAG 의 경우 가중치의 부호를 모두 바꾸어 최장 경로를 구할 수 있다.
- 다익스트라 알고리즘이나 벨만-포드 알고리즘은 이런 식으로 최장경로를 구할 수 없고, 최장결로를 구하는 것은 NP 문제에 해당하는 어려운 문제이다. 

```
DAG-ShortCut(G, r)

for each u in V:
    d[u] = inf
d[r] = 0

A <- 위상정렬된 G

for each u in A
    for each v in L(u) (=u 로부터 연결된 정점의 집합)
        if d[u] + w(u, v) < d[v]
            d[v] = d[u] + w(u, v)
            prev[v] = u
```


**강연결요소**

- 유향 그래프 G = (V, E) 에서 임의의 정점 u, v 에 대하여 경로 u -> v 와 v -> u 가 모두 존재하면 G 가 **강하게 연결**되어 있다고 표현한다.
- 그래프에서 강하게 연결되어 있는 부분 그래프를 강연결요소(Strongly Connected Component) 라고 한다.
- 임의의 그래프에서 강연결요소를 찾는 문제는 DFS 를 활용하는 응용 중 하나이다.

```
1. 그래프 G 에 대하여 임의의 정점으로부터 DFS 를 수행하고, 각 정점의 완료된 순서를 f[v] 에 기록한다.
1-1. 완료된 순서란 해당 정점과 연결된 간선을 모두 방문하여 해당 정점 이전의 정점을 탐색하게 되는 순서를 말한다.
2. G 의 모든 간선의 방향을 뒤집어 G^R 을 만든다.
3. f[v] 의 최대값이 존재하는 정점 v 에서부터 DFS(G^R) 을 수행하고 본인 정점을 포함한 경유한 정점의 집합을 기록한다.
4. 첫번째 강연결요소가 만들어졌다.
5. 이후 위 정점 집합에 포함되지 않은 정점 중 f[v] 최대값이 존재하는 정점 v 에서부터 위 강연결요소 정점 집합을 제외하고 DFS 를 수행한다.
6. 이렇게 경유하게 된 정점의 집합이 두번째 강연결요소이다.
7. 강연결요소 정점 집합의 합집합이 V 와 일치할 때까지 위 과정을 반복한다.
8. DFS 의 소요시간이 O(V + E) 이므로 강연결요소를 찾는 알고리즘의 소요시간은 O(V + E) 이다. 
```

---

### 10. 문자열 매칭

- 문자열 매칭은 텍스트 문자열이 패턴 문자열을 포함하고 있는지 알아보는 것이다.
- 이 때 패턴 문자열의 길이가 탐색되어야 하는 문자열의 길이보다 월등히 짧다고 가정한다.


**원시적인 매칭 방법**  

- 원시적인 방법으로 문자열의 각 자리마다 하나씩 확인하는 방법이 있다.
- O(nm) 의 시간이 소요된다.

```
n: 문자열 A[] 의 길이
m: 패턴 문자열 P[] 의 길이

for i in 1...(n-m+1)
    flag = TRUE
    for j in 1...m
        if A[i+j] != P[j]
            flag = FALSE
            break
    if flag
        i 번째에서 매칭이 발견됐음을 알림
```


**오토마타를 이용한 매칭**

- 오토마타는 여러 개의 상태로 표현되는데, 문제 해결 절차를 상태 간의 이동으로 나타낸 것이다.
- 오토마타의 수행 시간은 O(n) 이지만  상태 전이함수 테이블을 만드는 데에 시간이 소요되므로 총 수행 시간은 O(n + |S|m) 이다.
- 오토마타는 아래 다섯 가지로 구성된다.
- Q: 상태의 집합
- q: 오토마타의 작동이 시작되는 상태
- F: 목표상태
- S: 입력가능한 문자 집합
- e: 상태 전이 함수


- 예를 들어 알파벳 소문자만으로 구성된 문자열에서 문자열 ababaca 을 찾고자 할 때 위 다섯 가지 요소는 아래와 같이 정의된다.  
- Q = { 0, 1, 2, 3, 4, 5, 6, 7 }
- q = 0
- F = { 7 }
- S = { a, b, ..., y, z }
- e

| |a|b|c|...|y|z|  
|0|1|0|0|...|0|0|  
|1|1|2|0|...|0|0|  
|2|3|0|0|...|0|0|  
|3|1|4|0|...|0|0|  
...
|6|7|0|0|...|0|0|  

```
n: 문자열 A[] 의 길이

q <- 0
for i in 1...n
    q <- e(q, A[i])
    if q in F
        i 번째에서 매칭이 발견됐음을 알림
```


**라빈-카프 알고리즘**  

- 문자열 패턴을 수치로 바꾸어 비교한다.
- 가능한 문자 집합 S 의 크기에 따라 몇 진수를 쓸 것인지 결정된다.
- A 의 부분 문자열 A\[i ... i+m-1\] 에 대응되는 d진수 a_i 는 다음과 같다. a_i = A\[i+m-1\] + d\(A\[i+m-2\] + d\(A\[i+m-3\]...\)\)
- 점화식 a_\(i+1\) = A\[i+m\] + (da_i - 10^(m-1)A\[i\]) 가 성립한다.
- O(n) 의 시간이 소요된다.
- 문자 집합 S 의 크기와 m 의 길이에 따라 a_i 가 매우 큰 수가 되면 컴퓨터 레지스트리 용량을 초과하는 오버플로우가 발생할 수 있다.
- 따라서 위 연산에 module 연산을 수행하고, module 연산값이 같은 경우에만 추가적으로 확인하는 식으로 구현한다.  
- module 값이 같은 경우에 추가적인 시간이 소요되나 그러한 경우가 매우 적을 것으로 예상되므로 사실 상 O(n) 의 소요시간을 갖는다고 볼 수 있다.

```
n: 문자열 A[] 의 길이
m: 패턴 문자열 P[] 의 길이
d: d진수의 d
q: 충분히 큰 소수

p = 0
a = 0
for i in 1...m
    p <- (dp + P[i]) mod q
    a <- (da + A[i]) mod q

for i in 1...n-m+1
    if i != 1
        a <- (d(a - d^(m-1)A[i-1]) + A[i+m-1]) mod q
        if a == p
            if A[i...i+m-1] = P
                i 번째에서 매칭이 발견됐음을 알림
```


**KMP 알고리즘**  

- KMP 알고리즘은 패턴의 각 위치에 대해 매칭이 실패했을 때 돌아갈 곳을 알려주는 일차원 배열을 준비한다.
- 패턴에 대하여 오토마타를 만들어 두는 것과 유사하나 더욱 효율적이다.
- O(n + m) 의 시간이 소요된다.

```
n: 문자열 A[] 의 길이
m: 패턴 문자열 P[] 의 길이
T[m+1]: 해당 위치의 문자가 다를 경우 돌아갈 위치에 대한 배열

j <- 1
k <- 0
T[1] <- 0

while j <= m
    if k = 0 or P[j] = P[k]
        j++
        k++
        T[j] <- k
    else
        k <- T[k]

i <- 1
j <- 1
while i <= n
    if j = 0 or A[i] = P[j]
        i++
        j++
    else
        j <- T[j]

    if j = m + 1
        A[i - m] 번째에서 매칭이 발견됐음을 알림
        j <- T[j]
```


**보이어-무어 알고리즘**

- 이론적으로 최악의 상황에서 O(mn) 의 시간이 소요되지만 실제 사용에서는 라빈-카프나 KMP 알고리즘보다 성능이 뛰어나다.
- 최선의 상황, 즉 많은 경우에 대부분을 skip 한다면 O(n/m) 의 시간이 소요된다.
- 보이어-무어 알고리즘은 패턴 P\[m\] 의 문자가 A\[n\] 안의 특정 sub-string 에서 
아예 존재하지 않는다면 그만큼을 바로 뛰어넘어 체크하는 알고리즘이다.
- 예를 들어 A\[1...m\] 의 맨 오른쪽 문자 A\[m\] 이 패턴 P 에 존재하지 않는다면 A\[2\], ..., A\[m\] 은 확인할 필요도 없다.
- 따라서 A\[m+1\] 부터 검사를 하면 되므로 m + 1 만큼은 아예 검사하지 않고 뛰어넘은 것이다.  
- 위 처리 부분이 까다로워서 약식으로 처리한 보이어-무어-호스풀 알고리즘이 제시되었다.  
- 보이어-무어-호스풀 알고리즘은 아래와 같다.
- 보이어-무어-호스풀과 보이어-무어의 차이점은 Jump 를 할 때에 해당 문자만 고려하는지, 불일치 문자와 패턴 내의 불일치 인덱스를 함께 비교하는지이다.

```
n: 문자열 A[] 의 길이
m: 패턴 문자열 P[] 의 길이
Jump: S->N 의 함수배열

for s in S
    Jump[s] <- m

# 같은 문자가 존재하는 경우 작은 값으로 결정된다.
for (i, p) in (m, P[m]) ... (1, P[1])
    if i = m
        Jump[s] <- m
    else
        Jump[s] <- m - i

i <- 1
while i <= n - m + 1
    j <- m
    k <- i + m - 1
    while j > 0 and P[j] = A[k]
        j--
        k--

    if j = 0
        i 번째에서 매칭이 발견되었음을 알린다.
    
    i += Jump[A[i+m-1]]
```